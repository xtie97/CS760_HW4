{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def read_txt(txt_file: str):\n",
    "    # read all words and replace '\\n'\n",
    "    with open(txt_file, \"r\") as data:\n",
    "        txt = data.read()\n",
    "    return txt.replace('\\n', '').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the English documents: 15119\n",
      "Total number of characters in the Japanese documents: 14296\n",
      "Total number of characters in the Spanish documents: 16202\n",
      "Total number of characters: 45617\n"
     ]
    }
   ],
   "source": [
    "eng_cha = ''\n",
    "jap_cha = ''\n",
    "spa_cha = ''\n",
    "num_train_documents = 10 \n",
    "\n",
    "for ii in range(num_train_documents):\n",
    "    txt_file = './languageID/{}.txt'.format('e'+str(ii))\n",
    "    eng_cha += read_txt(txt_file)\n",
    "    \n",
    "for ii in range(num_train_documents):\n",
    "    txt_file = './languageID/{}.txt'.format('j'+str(ii))\n",
    "    jap_cha += read_txt(txt_file)\n",
    "    \n",
    "for ii in range(num_train_documents):\n",
    "    txt_file = './languageID/{}.txt'.format('s'+str(ii))\n",
    "    spa_cha += read_txt(txt_file)\n",
    "\n",
    "print('Total number of characters in the English documents: {}'.format(len(eng_cha)))\n",
    "print('Total number of characters in the Japanese documents: {}'.format(len(jap_cha)))\n",
    "print('Total number of characters in the Spanish documents: {}'.format(len(spa_cha)))\n",
    "print('Total number of characters: {}'.format(len(eng_cha+jap_cha+spa_cha)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_e is [0.0601685114819098, 0.011134974392863043, 0.021509995043779945, 0.021972575582355856, 0.1053692383941847, 0.018932760614571286, 0.017478936064761277, 0.047216256401784236, 0.055410540227986124, 0.001420783082768875, 0.0037336857756484387, 0.028977366595076822, 0.020518751032545846, 0.057921691723112505, 0.06446390219725756, 0.01675202378985627, 0.0005617049396993227, 0.053824549810011564, 0.06618205848339666, 0.08012555757475633, 0.026664463902197257, 0.009284652238559392, 0.015496448042293078, 0.001156451346439782, 0.013844374690236246, 0.0006277878737815959, 0.1792499586981662]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "cha_list = list(string.ascii_lowercase) + [' ']\n",
    "\n",
    "Ks = 27\n",
    "Kl = 3\n",
    "alpha = 1/2 \n",
    "j = ['a', 'b']\n",
    "ccp_e = [] \n",
    "for cha in cha_list:\n",
    "    cha_occurance = len([i for i in eng_cha if i==cha])\n",
    "    ccp_e.append((cha_occurance+alpha)/(len(eng_cha)+Ks*alpha))\n",
    "print('theta_e is {}'.format(ccp_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_j is [0.1317656102589189, 0.010866906600510151, 0.005485866033054963, 0.01722631818022992, 0.06020475907613823, 0.003878542227191726, 0.014011670568503443, 0.03176211607673224, 0.09703343932352633, 0.0023411020650616725, 0.05740941332681086, 0.001432614696530277, 0.03979873510604843, 0.05671057688947902, 0.09116321324993885, 0.0008735455466648031, 0.00010482546559977637, 0.04280373178657535, 0.0421747789929767, 0.056990111464411755, 0.07061742199238269, 0.0002445927530661449, 0.01974212935462455, 3.4941821866592126e-05, 0.01415143785596981, 0.00772214263251686, 0.12344945665466997]\n",
      "theta_s is [0.10456045141993771, 0.008232863618143134, 0.03752582405722919, 0.039745922111559924, 0.1138108599796491, 0.00860287996053159, 0.0071844839813758445, 0.0045327001942585795, 0.049859702136844375, 0.006629459467793161, 0.0002775122567913416, 0.052943171656748174, 0.02580863988159477, 0.054176559464709693, 0.07249236841293824, 0.02426690512164287, 0.007677839104560451, 0.05929511886774999, 0.06577040485954797, 0.03561407295488884, 0.03370232185254849, 0.00588942678301625, 9.250408559711388e-05, 0.0024976103111220747, 0.007862847275754679, 0.0026826184823163022, 0.16826493170115014]\n"
     ]
    }
   ],
   "source": [
    "ccp_j = [] \n",
    "for cha in cha_list:\n",
    "    cha_occurance = len([i for i in jap_cha if i==cha])\n",
    "    ccp_j.append((cha_occurance+alpha)/(len(jap_cha)+Ks*alpha))\n",
    "print('theta_j is {}'.format(ccp_j))\n",
    " \n",
    "ccp_s = [] \n",
    "for cha in cha_list:\n",
    "    cha_occurance = len([i for i in spa_cha if i==cha])\n",
    "    ccp_s.append((cha_occurance+alpha)/(len(spa_cha)+Ks*alpha))\n",
    "print('theta_s is {}'.format(ccp_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag-of-words count vector is [164, 32, 53, 57, 311, 55, 51, 140, 140, 3, 6, 85, 64, 139, 182, 53, 3, 141, 186, 225, 65, 31, 47, 4, 38, 2, 498]\n"
     ]
    }
   ],
   "source": [
    "txt_file = './languageID/{}.txt'.format('e10')\n",
    "test_txt = read_txt(txt_file)\n",
    "\n",
    "wcv = [] # word count vector \n",
    "for cha in cha_list:\n",
    "    cha_occurance = len([i for i in test_txt if i==cha])\n",
    "    wcv.append(cha_occurance)\n",
    "print('bag-of-words count vector is {}'.format(wcv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log of conditional probability p(x|y=English) is -7841.9\n",
      "log of conditional probability p(x|y=Japanese) is -8771.4\n",
      "log of conditional probability p(x|y=Spanish) is -8467.3\n"
     ]
    }
   ],
   "source": [
    "lnpx_e = np.sum(wcv * np.log(ccp_e))\n",
    "lnpx_j = np.sum(wcv * np.log(ccp_j))\n",
    "lnpx_s = np.sum(wcv * np.log(ccp_s))\n",
    "\n",
    "print('log of conditional probability p(x|y=English) is {:.1f}'.format(lnpx_e))\n",
    "print('log of conditional probability p(x|y=Japanese) is {:.1f}'.format(lnpx_j))\n",
    "print('log of conditional probability p(x|y=Spanish) is {:.1f}'.format(lnpx_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log of posterior probability p(y=English|x) is -7843.0\n",
      "log of posterior probability p(y=Japanese|x) is -8772.5\n",
      "log of posterior probability p(y=Spanish|x) is -8468.4\n"
     ]
    }
   ],
   "source": [
    "lnpe_x = lnpx_e + np.log(1/3) \n",
    "lnpj_x = lnpx_j + np.log(1/3) \n",
    "lnps_x = lnpx_s + np.log(1/3) \n",
    "\n",
    "print('log of posterior probability p(y=English|x) is {:.1f}'.format(lnpe_x))\n",
    "print('log of posterior probability p(y=Japanese|x) is {:.1f}'.format(lnpj_x))\n",
    "print('log of posterior probability p(y=Spanish|x) is {:.1f}'.format(lnps_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English \n",
    "pred_class_english = []\n",
    "for ii in np.arange(10, 20):\n",
    "    txt_file = './languageID/e{}.txt'.format(ii)\n",
    "    test_txt = read_txt(txt_file)\n",
    "\n",
    "    wcv = [] # word count vector \n",
    "    for cha in cha_list:\n",
    "        cha_occurance = len([i for i in test_txt if i==cha])\n",
    "        wcv.append(cha_occurance)\n",
    "    lnpe_x = np.sum(wcv * np.log(ccp_e)) + np.log(1/3) \n",
    "    lnpj_x = np.sum(wcv * np.log(ccp_j)) + np.log(1/3) \n",
    "    lnps_x = np.sum(wcv * np.log(ccp_s)) + np.log(1/3) \n",
    "    pred_class_english.append(np.argmax([lnpe_x, lnpj_x, lnps_x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Japanese \n",
    "pred_class_japanese = []\n",
    "for ii in np.arange(10, 20):\n",
    "    txt_file = './languageID/j{}.txt'.format(ii)\n",
    "    test_txt = read_txt(txt_file)\n",
    "\n",
    "    wcv = [] # word count vector \n",
    "    for cha in cha_list:\n",
    "        cha_occurance = len([i for i in test_txt if i==cha])\n",
    "        wcv.append(cha_occurance)\n",
    "    lnpe_x = np.sum(wcv * np.log(ccp_e)) + np.log(1/3) \n",
    "    lnpj_x = np.sum(wcv * np.log(ccp_j)) + np.log(1/3) \n",
    "    lnps_x = np.sum(wcv * np.log(ccp_s)) + np.log(1/3) \n",
    "    pred_class_japanese.append(np.argmax([lnpe_x, lnpj_x, lnps_x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish \n",
    "pred_class_spanish = []\n",
    "for ii in np.arange(10, 20):\n",
    "    txt_file = './languageID/s{}.txt'.format(ii)\n",
    "    test_txt = read_txt(txt_file)\n",
    "\n",
    "    wcv = [] # word count vector \n",
    "    for cha in cha_list:\n",
    "        cha_occurance = len([i for i in test_txt if i==cha])\n",
    "        wcv.append(cha_occurance)\n",
    "    lnpe_x = np.sum(wcv * np.log(ccp_e)) + np.log(1/3) \n",
    "    lnpj_x = np.sum(wcv * np.log(ccp_j)) + np.log(1/3) \n",
    "    lnps_x = np.sum(wcv * np.log(ccp_s)) + np.log(1/3) \n",
    "    pred_class_spanish.append(np.argmax([lnpe_x, lnpj_x, lnps_x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  0,  0],\n",
       "       [ 0, 10,  0],\n",
       "       [ 0,  0, 10]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [0] * 10 + [2] * 10 + [1] * 10\n",
    "y_pred = pred_class_english + pred_class_spanish + pred_class_japanese\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import random_split, DataLoader \n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pylab as plt \n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "# load mnist dataset\n",
    "root = './data/'\n",
    "train_set = datasets.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = datasets.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "\n",
    "batch_size = 1024\n",
    "nepoch = 100\n",
    "train_subset, val_subset = random_split(\n",
    "        train_set, [0.9, 0.1], generator=torch.Generator().manual_seed(1))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "                 dataset=train_subset,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True, drop_last=False)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "                 dataset=val_subset,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True, drop_last=False)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-implemented version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network\n",
    "class NN_self(): \n",
    "    def __init__(self, d1, d2, alpha, out_class=10):\n",
    "        self.W1 = torch.rand(d1, 28*28) * 2 - 1\n",
    "        self.W2 = torch.rand(d2, d1) * 2 - 1 \n",
    "        self.W3 = torch.rand(out_class, d2) * 2 - 1\n",
    "        self.b1 = torch.zeros(1, d1)\n",
    "        self.b2 = torch.zeros(1, d2)\n",
    "        self.b3 = torch.zeros(1, out_class)\n",
    "        self.alpha = alpha # learning rate \n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "       \n",
    "    def softmax(self, x):\n",
    "        beta = torch.max(x, dim=1, keepdim=True)[0] # avoid overflow \n",
    "        x_exp = torch.exp(x-beta)\n",
    "        x_exp_sum = torch.sum(x_exp, 1, keepdim=True)\n",
    "        return x_exp / x_exp_sum\n",
    "    \n",
    "    def cross_entropy(self, yhat, ytrue):\n",
    "        return - yhat[range(ytrue.shape[0]), ytrue].log().mean()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28) # batch*(28*28)\n",
    "        z1 = self.sigmoid(torch.matmul(x, self.W1.T) + self.b1) \n",
    "        z2 = self.sigmoid(torch.matmul(z1, self.W2.T) + self.b2) \n",
    "        out = self.softmax(torch.matmul(z2, self.W3.T) + self.b3)\n",
    "        return out, z2, z1, x\n",
    "    \n",
    "    def backward(self, out, ref, z2, z1, x):\n",
    "        batchsize = out.shape[0]\n",
    "        dz3 = - (ref - out) # z3 = output \n",
    "        db3 = torch.mean(dz3, dim=0, keepdim=True) \n",
    "        dW3 = torch.matmul(dz3.T, z2) / batchsize # normalized by the batch size \n",
    "        \n",
    "        dz2 = torch.matmul(dz3, self.W3) * z2 * (1 - z2)\n",
    "        db2 = torch.mean(dz2, dim=0, keepdim=True)\n",
    "        dW2 = torch.matmul(dz2.T, z1) / batchsize\n",
    "        \n",
    "        dz1 = torch.matmul(dz2, self.W2) * z1 * (1 - z1)\n",
    "        db1 = torch.mean(dz1, dim=0, keepdim=True)\n",
    "        dW1 = torch.matmul(dz1.T, x) / batchsize\n",
    "        \n",
    "        self.b3 -= self.alpha * db3\n",
    "        self.W3 -= self.alpha * dW3\n",
    "        self.b2 -= self.alpha * db2\n",
    "        self.W2 -= self.alpha * dW2\n",
    "        self.b1 -= self.alpha * db1\n",
    "        self.W1 -= self.alpha * dW1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, train loss: 2.128\n",
      "==>>> epoch: 0, validation loss: 1.320, accuracy: 58.150%\n",
      "==>>> epoch: 1, train loss: 1.079\n",
      "==>>> epoch: 1, validation loss: 0.932, accuracy: 70.267%\n",
      "==>>> epoch: 2, train loss: 0.829\n",
      "==>>> epoch: 2, validation loss: 0.780, accuracy: 75.550%\n",
      "==>>> epoch: 3, train loss: 0.705\n",
      "==>>> epoch: 3, validation loss: 0.688, accuracy: 78.417%\n",
      "==>>> epoch: 4, train loss: 0.627\n",
      "==>>> epoch: 4, validation loss: 0.625, accuracy: 80.350%\n",
      "==>>> epoch: 5, train loss: 0.572\n",
      "==>>> epoch: 5, validation loss: 0.581, accuracy: 81.750%\n",
      "==>>> epoch: 6, train loss: 0.530\n",
      "==>>> epoch: 6, validation loss: 0.550, accuracy: 82.783%\n",
      "==>>> epoch: 7, train loss: 0.497\n",
      "==>>> epoch: 7, validation loss: 0.531, accuracy: 83.367%\n",
      "==>>> epoch: 8, train loss: 0.471\n",
      "==>>> epoch: 8, validation loss: 0.503, accuracy: 84.167%\n",
      "==>>> epoch: 9, train loss: 0.448\n",
      "==>>> epoch: 9, validation loss: 0.486, accuracy: 84.850%\n",
      "==>>> epoch: 10, train loss: 0.428\n",
      "==>>> epoch: 10, validation loss: 0.470, accuracy: 85.267%\n",
      "==>>> epoch: 11, train loss: 0.411\n",
      "==>>> epoch: 11, validation loss: 0.456, accuracy: 85.833%\n",
      "==>>> epoch: 12, train loss: 0.396\n",
      "==>>> epoch: 12, validation loss: 0.444, accuracy: 86.133%\n",
      "==>>> epoch: 13, train loss: 0.382\n",
      "==>>> epoch: 13, validation loss: 0.435, accuracy: 86.600%\n",
      "==>>> epoch: 14, train loss: 0.370\n",
      "==>>> epoch: 14, validation loss: 0.423, accuracy: 86.933%\n",
      "==>>> epoch: 15, train loss: 0.359\n",
      "==>>> epoch: 15, validation loss: 0.419, accuracy: 86.750%\n",
      "==>>> epoch: 16, train loss: 0.349\n",
      "==>>> epoch: 16, validation loss: 0.407, accuracy: 87.200%\n",
      "==>>> epoch: 17, train loss: 0.339\n",
      "==>>> epoch: 17, validation loss: 0.400, accuracy: 87.733%\n",
      "==>>> epoch: 18, train loss: 0.331\n",
      "==>>> epoch: 18, validation loss: 0.394, accuracy: 88.000%\n",
      "==>>> epoch: 19, train loss: 0.323\n",
      "==>>> epoch: 19, validation loss: 0.389, accuracy: 88.017%\n",
      "==>>> epoch: 20, train loss: 0.315\n",
      "==>>> epoch: 20, validation loss: 0.385, accuracy: 88.100%\n",
      "==>>> epoch: 21, train loss: 0.308\n",
      "==>>> epoch: 21, validation loss: 0.378, accuracy: 88.317%\n",
      "==>>> epoch: 22, train loss: 0.301\n",
      "==>>> epoch: 22, validation loss: 0.375, accuracy: 88.217%\n",
      "==>>> epoch: 23, train loss: 0.295\n",
      "==>>> epoch: 23, validation loss: 0.367, accuracy: 88.717%\n",
      "==>>> epoch: 24, train loss: 0.289\n",
      "==>>> epoch: 24, validation loss: 0.363, accuracy: 88.983%\n",
      "==>>> epoch: 25, train loss: 0.283\n",
      "==>>> epoch: 25, validation loss: 0.362, accuracy: 88.867%\n",
      "==>>> epoch: 26, train loss: 0.278\n",
      "==>>> epoch: 26, validation loss: 0.356, accuracy: 89.100%\n",
      "==>>> epoch: 27, train loss: 0.273\n",
      "==>>> epoch: 27, validation loss: 0.355, accuracy: 89.067%\n",
      "==>>> epoch: 28, train loss: 0.268\n",
      "==>>> epoch: 28, validation loss: 0.349, accuracy: 89.333%\n",
      "==>>> epoch: 29, train loss: 0.263\n",
      "==>>> epoch: 29, validation loss: 0.346, accuracy: 89.317%\n",
      "==>>> epoch: 30, train loss: 0.259\n",
      "==>>> epoch: 30, validation loss: 0.342, accuracy: 89.450%\n",
      "==>>> epoch: 31, train loss: 0.254\n",
      "==>>> epoch: 31, validation loss: 0.337, accuracy: 89.667%\n",
      "==>>> epoch: 32, train loss: 0.250\n",
      "==>>> epoch: 32, validation loss: 0.337, accuracy: 89.550%\n",
      "==>>> epoch: 33, train loss: 0.246\n",
      "==>>> epoch: 33, validation loss: 0.334, accuracy: 89.783%\n",
      "==>>> epoch: 34, train loss: 0.242\n",
      "==>>> epoch: 34, validation loss: 0.331, accuracy: 89.733%\n",
      "==>>> epoch: 35, train loss: 0.238\n",
      "==>>> epoch: 35, validation loss: 0.328, accuracy: 89.800%\n",
      "==>>> epoch: 36, train loss: 0.234\n",
      "==>>> epoch: 36, validation loss: 0.326, accuracy: 90.083%\n",
      "==>>> epoch: 37, train loss: 0.231\n",
      "==>>> epoch: 37, validation loss: 0.323, accuracy: 90.200%\n",
      "==>>> epoch: 38, train loss: 0.227\n",
      "==>>> epoch: 38, validation loss: 0.323, accuracy: 90.083%\n",
      "==>>> epoch: 39, train loss: 0.224\n",
      "==>>> epoch: 39, validation loss: 0.319, accuracy: 90.300%\n",
      "==>>> epoch: 40, train loss: 0.221\n",
      "==>>> epoch: 40, validation loss: 0.316, accuracy: 90.400%\n",
      "==>>> epoch: 41, train loss: 0.218\n",
      "==>>> epoch: 41, validation loss: 0.314, accuracy: 90.517%\n",
      "==>>> epoch: 42, train loss: 0.215\n",
      "==>>> epoch: 42, validation loss: 0.314, accuracy: 90.467%\n",
      "==>>> epoch: 43, train loss: 0.212\n",
      "==>>> epoch: 43, validation loss: 0.311, accuracy: 90.550%\n",
      "==>>> epoch: 44, train loss: 0.209\n",
      "==>>> epoch: 44, validation loss: 0.310, accuracy: 90.633%\n",
      "==>>> epoch: 45, train loss: 0.206\n",
      "==>>> epoch: 45, validation loss: 0.308, accuracy: 90.700%\n",
      "==>>> epoch: 46, train loss: 0.203\n",
      "==>>> epoch: 46, validation loss: 0.307, accuracy: 90.683%\n",
      "==>>> epoch: 47, train loss: 0.201\n",
      "==>>> epoch: 47, validation loss: 0.306, accuracy: 90.700%\n",
      "==>>> epoch: 48, train loss: 0.198\n",
      "==>>> epoch: 48, validation loss: 0.304, accuracy: 90.750%\n",
      "==>>> epoch: 49, train loss: 0.195\n",
      "==>>> epoch: 49, validation loss: 0.302, accuracy: 90.800%\n",
      "==>>> epoch: 50, train loss: 0.193\n",
      "==>>> epoch: 50, validation loss: 0.298, accuracy: 90.800%\n",
      "==>>> epoch: 51, train loss: 0.191\n",
      "==>>> epoch: 51, validation loss: 0.300, accuracy: 90.883%\n",
      "==>>> epoch: 52, train loss: 0.188\n",
      "==>>> epoch: 52, validation loss: 0.297, accuracy: 90.950%\n",
      "==>>> epoch: 53, train loss: 0.186\n",
      "==>>> epoch: 53, validation loss: 0.297, accuracy: 90.833%\n",
      "==>>> epoch: 54, train loss: 0.184\n",
      "==>>> epoch: 54, validation loss: 0.294, accuracy: 91.133%\n",
      "==>>> epoch: 55, train loss: 0.182\n",
      "==>>> epoch: 55, validation loss: 0.293, accuracy: 91.050%\n",
      "==>>> epoch: 56, train loss: 0.179\n",
      "==>>> epoch: 56, validation loss: 0.292, accuracy: 91.283%\n",
      "==>>> epoch: 57, train loss: 0.177\n",
      "==>>> epoch: 57, validation loss: 0.292, accuracy: 91.183%\n",
      "==>>> epoch: 58, train loss: 0.175\n",
      "==>>> epoch: 58, validation loss: 0.290, accuracy: 91.267%\n",
      "==>>> epoch: 59, train loss: 0.173\n",
      "==>>> epoch: 59, validation loss: 0.288, accuracy: 91.333%\n",
      "==>>> epoch: 60, train loss: 0.171\n",
      "==>>> epoch: 60, validation loss: 0.288, accuracy: 91.383%\n",
      "==>>> epoch: 61, train loss: 0.169\n",
      "==>>> epoch: 61, validation loss: 0.287, accuracy: 91.350%\n",
      "==>>> epoch: 62, train loss: 0.167\n",
      "==>>> epoch: 62, validation loss: 0.286, accuracy: 91.300%\n",
      "==>>> epoch: 63, train loss: 0.165\n",
      "==>>> epoch: 63, validation loss: 0.284, accuracy: 91.383%\n",
      "==>>> epoch: 64, train loss: 0.164\n",
      "==>>> epoch: 64, validation loss: 0.284, accuracy: 91.433%\n",
      "==>>> epoch: 65, train loss: 0.162\n",
      "==>>> epoch: 65, validation loss: 0.283, accuracy: 91.450%\n",
      "==>>> epoch: 66, train loss: 0.160\n",
      "==>>> epoch: 66, validation loss: 0.282, accuracy: 91.450%\n",
      "==>>> epoch: 67, train loss: 0.159\n",
      "==>>> epoch: 67, validation loss: 0.280, accuracy: 91.500%\n",
      "==>>> epoch: 68, train loss: 0.156\n",
      "==>>> epoch: 68, validation loss: 0.279, accuracy: 91.567%\n",
      "==>>> epoch: 69, train loss: 0.155\n",
      "==>>> epoch: 69, validation loss: 0.279, accuracy: 91.517%\n",
      "==>>> epoch: 70, train loss: 0.153\n",
      "==>>> epoch: 70, validation loss: 0.278, accuracy: 91.517%\n",
      "==>>> epoch: 71, train loss: 0.152\n",
      "==>>> epoch: 71, validation loss: 0.276, accuracy: 91.683%\n",
      "==>>> epoch: 72, train loss: 0.150\n",
      "==>>> epoch: 72, validation loss: 0.277, accuracy: 91.683%\n",
      "==>>> epoch: 73, train loss: 0.148\n",
      "==>>> epoch: 73, validation loss: 0.276, accuracy: 91.683%\n",
      "==>>> epoch: 74, train loss: 0.147\n",
      "==>>> epoch: 74, validation loss: 0.274, accuracy: 91.583%\n",
      "==>>> epoch: 75, train loss: 0.146\n",
      "==>>> epoch: 75, validation loss: 0.273, accuracy: 91.783%\n",
      "==>>> epoch: 76, train loss: 0.144\n",
      "==>>> epoch: 76, validation loss: 0.272, accuracy: 91.767%\n",
      "==>>> epoch: 77, train loss: 0.143\n",
      "==>>> epoch: 77, validation loss: 0.273, accuracy: 91.733%\n",
      "==>>> epoch: 78, train loss: 0.141\n",
      "==>>> epoch: 78, validation loss: 0.272, accuracy: 91.783%\n",
      "==>>> epoch: 79, train loss: 0.140\n",
      "==>>> epoch: 79, validation loss: 0.272, accuracy: 91.850%\n",
      "==>>> epoch: 80, train loss: 0.138\n",
      "==>>> epoch: 80, validation loss: 0.269, accuracy: 91.767%\n",
      "==>>> epoch: 81, train loss: 0.137\n",
      "==>>> epoch: 81, validation loss: 0.271, accuracy: 91.817%\n",
      "==>>> epoch: 82, train loss: 0.136\n",
      "==>>> epoch: 82, validation loss: 0.268, accuracy: 91.867%\n",
      "==>>> epoch: 83, train loss: 0.134\n",
      "==>>> epoch: 83, validation loss: 0.270, accuracy: 91.917%\n",
      "==>>> epoch: 84, train loss: 0.133\n",
      "==>>> epoch: 84, validation loss: 0.267, accuracy: 91.950%\n",
      "==>>> epoch: 85, train loss: 0.132\n",
      "==>>> epoch: 85, validation loss: 0.268, accuracy: 91.933%\n",
      "==>>> epoch: 86, train loss: 0.130\n",
      "==>>> epoch: 86, validation loss: 0.266, accuracy: 92.083%\n",
      "==>>> epoch: 87, train loss: 0.129\n",
      "==>>> epoch: 87, validation loss: 0.266, accuracy: 91.983%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 88, train loss: 0.128\n",
      "==>>> epoch: 88, validation loss: 0.265, accuracy: 92.033%\n",
      "==>>> epoch: 89, train loss: 0.127\n",
      "==>>> epoch: 89, validation loss: 0.266, accuracy: 92.067%\n",
      "==>>> epoch: 90, train loss: 0.125\n",
      "==>>> epoch: 90, validation loss: 0.263, accuracy: 92.050%\n",
      "==>>> epoch: 91, train loss: 0.124\n",
      "==>>> epoch: 91, validation loss: 0.263, accuracy: 92.150%\n",
      "==>>> epoch: 92, train loss: 0.123\n",
      "==>>> epoch: 92, validation loss: 0.264, accuracy: 92.167%\n",
      "==>>> epoch: 93, train loss: 0.122\n",
      "==>>> epoch: 93, validation loss: 0.264, accuracy: 92.150%\n",
      "==>>> epoch: 94, train loss: 0.121\n",
      "==>>> epoch: 94, validation loss: 0.263, accuracy: 92.233%\n",
      "==>>> epoch: 95, train loss: 0.120\n",
      "==>>> epoch: 95, validation loss: 0.261, accuracy: 92.133%\n",
      "==>>> epoch: 96, train loss: 0.119\n",
      "==>>> epoch: 96, validation loss: 0.261, accuracy: 92.167%\n",
      "==>>> epoch: 97, train loss: 0.118\n",
      "==>>> epoch: 97, validation loss: 0.261, accuracy: 92.150%\n",
      "==>>> epoch: 98, train loss: 0.116\n",
      "==>>> epoch: 98, validation loss: 0.260, accuracy: 92.217%\n",
      "==>>> epoch: 99, train loss: 0.116\n",
      "==>>> epoch: 99, validation loss: 0.258, accuracy: 92.250%\n"
     ]
    }
   ],
   "source": [
    "acc_max = 0.0\n",
    "#criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "## training\n",
    "model_self = NN_self(300, 200, alpha=0.2) # alpha/learning rate \n",
    "train_loss_epochs = []\n",
    "val_loss_epochs = [] \n",
    "val_acc_epochs = []\n",
    "for epoch in range(nepoch):\n",
    "    # trainning\n",
    "    train_loss = 0\n",
    "    count = 0 \n",
    "    for x, target in train_loader:\n",
    "        out, z2, z1, x = model_self.forward(x)\n",
    "        loss = model_self.cross_entropy(out, target) \n",
    "        #loss = F.cross_entropy(out.log(), target) # cross entropy loss accepts logits rather than softmax output \n",
    "        train_loss += loss\n",
    "        # to one-hot \n",
    "        ref = F.one_hot(target, num_classes=10).to(out)\n",
    "        model_self.backward(out, ref, z2, z1, x)\n",
    "        count += 1\n",
    "        \n",
    "    train_loss /= count \n",
    "    # validation\n",
    "    val_loss = 0\n",
    "    correct_count = 0\n",
    "    tot_count = 0 \n",
    "    count = 0 \n",
    "    for x, target in val_loader:\n",
    "        out, _, _, _ = model_self.forward(x) \n",
    "        loss = model_self.cross_entropy(out, target) \n",
    "        val_loss += loss\n",
    "        count += 1\n",
    "        _, pred_label = torch.max(out, 1)\n",
    "        correct_count += (pred_label == target.data).sum()\n",
    "        tot_count += pred_label.shape[0]\n",
    "    \n",
    "    val_loss /= count \n",
    "    acc = correct_count / tot_count * 100 \n",
    "    print('==>>> epoch: {}, train loss: {:.3f}'.format(epoch, train_loss))\n",
    "    print('==>>> epoch: {}, validation loss: {:.3f}, accuracy: {:.3f}%'.format(epoch, val_loss, acc)) \n",
    "    \n",
    "    train_loss_epochs.append(train_loss)\n",
    "    val_loss_epochs.append(val_loss)\n",
    "    val_acc_epochs.append(acc)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(train_loss_epochs, label='Training loss')\n",
    "    plt.plot(val_loss_epochs, label='Validation loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.rc('font', size=18) # controls default text sizes\n",
    "\n",
    "    plt.savefig('./figures/model_self_loss_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(val_acc_epochs, label='Validation accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.rc('font', size=18)\n",
    "    plt.savefig('./figures/model_self_accuracy_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    if acc > acc_max:\n",
    "        acc_max = acc \n",
    "        torch.save(model_self, './model_weights/model_self_best.pth.tar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> Testing accuracy 92.620%\n",
      "==>>> Testing error 7.380%\n"
     ]
    }
   ],
   "source": [
    "# Testing     \n",
    "# To load model weights\n",
    "model_self = torch.load('./model_weights/model_self_best.pth.tar')\n",
    "correct_count = 0\n",
    "wrong_count = 0\n",
    "tot_count = 0\n",
    "for x, target in test_loader:\n",
    "    out, _, _, _ = model_self.forward(x) \n",
    "    _, pred_label = torch.max(out, 1)\n",
    "    correct_count += (pred_label == target.data).sum()\n",
    "    wrong_count += (pred_label != target.data).sum() \n",
    "    tot_count += pred_label.shape[0]\n",
    "    \n",
    "test_acc = correct_count / tot_count * 100 \n",
    "test_err = wrong_count / tot_count * 100 \n",
    "print('==>>> Testing accuracy {:.3f}%'.format(test_acc)) \n",
    "print('==>>> Testing error {:.3f}%'.format(test_err)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        #nn.init.zeros_(tensor)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# network\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, d1, d2, out_class=10):\n",
    "        super(NN, self).__init__()\n",
    "        model = nn.Sequential(nn.Linear(28*28, d1),\n",
    "                                   nn.Sigmoid(),\n",
    "                                   nn.Linear(d1, d2), \n",
    "                                   nn.Sigmoid(),\n",
    "                                   nn.Linear(d2, out_class), \n",
    "                                  )\n",
    "        self.model = model.apply(init_weights)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, train loss: 2.019\n",
      "==>>> epoch: 0, validation loss: 1.084, accuracy: 58.033%\n",
      "==>>> epoch: 1, train loss: 0.784\n",
      "==>>> epoch: 1, validation loss: 0.515, accuracy: 84.150%\n",
      "==>>> epoch: 2, train loss: 0.415\n",
      "==>>> epoch: 2, validation loss: 0.346, accuracy: 90.367%\n",
      "==>>> epoch: 3, train loss: 0.333\n",
      "==>>> epoch: 3, validation loss: 0.300, accuracy: 91.317%\n",
      "==>>> epoch: 4, train loss: 0.280\n",
      "==>>> epoch: 4, validation loss: 0.270, accuracy: 92.250%\n",
      "==>>> epoch: 5, train loss: 0.248\n",
      "==>>> epoch: 5, validation loss: 0.236, accuracy: 92.983%\n",
      "==>>> epoch: 6, train loss: 0.221\n",
      "==>>> epoch: 6, validation loss: 0.221, accuracy: 93.300%\n",
      "==>>> epoch: 7, train loss: 0.197\n",
      "==>>> epoch: 7, validation loss: 0.195, accuracy: 94.200%\n",
      "==>>> epoch: 8, train loss: 0.179\n",
      "==>>> epoch: 8, validation loss: 0.179, accuracy: 94.467%\n",
      "==>>> epoch: 9, train loss: 0.161\n",
      "==>>> epoch: 9, validation loss: 0.167, accuracy: 95.167%\n",
      "==>>> epoch: 10, train loss: 0.149\n",
      "==>>> epoch: 10, validation loss: 0.156, accuracy: 95.083%\n",
      "==>>> epoch: 11, train loss: 0.137\n",
      "==>>> epoch: 11, validation loss: 0.147, accuracy: 95.500%\n",
      "==>>> epoch: 12, train loss: 0.126\n",
      "==>>> epoch: 12, validation loss: 0.149, accuracy: 95.400%\n",
      "==>>> epoch: 13, train loss: 0.117\n",
      "==>>> epoch: 13, validation loss: 0.139, accuracy: 95.750%\n",
      "==>>> epoch: 14, train loss: 0.109\n",
      "==>>> epoch: 14, validation loss: 0.130, accuracy: 96.067%\n",
      "==>>> epoch: 15, train loss: 0.102\n",
      "==>>> epoch: 15, validation loss: 0.122, accuracy: 96.317%\n",
      "==>>> epoch: 16, train loss: 0.094\n",
      "==>>> epoch: 16, validation loss: 0.116, accuracy: 96.550%\n",
      "==>>> epoch: 17, train loss: 0.089\n",
      "==>>> epoch: 17, validation loss: 0.114, accuracy: 96.400%\n",
      "==>>> epoch: 18, train loss: 0.083\n",
      "==>>> epoch: 18, validation loss: 0.111, accuracy: 96.667%\n",
      "==>>> epoch: 19, train loss: 0.078\n",
      "==>>> epoch: 19, validation loss: 0.103, accuracy: 96.867%\n",
      "==>>> epoch: 20, train loss: 0.074\n",
      "==>>> epoch: 20, validation loss: 0.102, accuracy: 96.767%\n",
      "==>>> epoch: 21, train loss: 0.071\n",
      "==>>> epoch: 21, validation loss: 0.099, accuracy: 96.917%\n",
      "==>>> epoch: 22, train loss: 0.066\n",
      "==>>> epoch: 22, validation loss: 0.096, accuracy: 97.033%\n",
      "==>>> epoch: 23, train loss: 0.062\n",
      "==>>> epoch: 23, validation loss: 0.097, accuracy: 96.933%\n",
      "==>>> epoch: 24, train loss: 0.058\n",
      "==>>> epoch: 24, validation loss: 0.095, accuracy: 97.033%\n",
      "==>>> epoch: 25, train loss: 0.054\n",
      "==>>> epoch: 25, validation loss: 0.091, accuracy: 97.117%\n",
      "==>>> epoch: 26, train loss: 0.052\n",
      "==>>> epoch: 26, validation loss: 0.086, accuracy: 97.417%\n",
      "==>>> epoch: 27, train loss: 0.049\n",
      "==>>> epoch: 27, validation loss: 0.084, accuracy: 97.433%\n",
      "==>>> epoch: 28, train loss: 0.046\n",
      "==>>> epoch: 28, validation loss: 0.084, accuracy: 97.450%\n",
      "==>>> epoch: 29, train loss: 0.044\n",
      "==>>> epoch: 29, validation loss: 0.082, accuracy: 97.367%\n",
      "==>>> epoch: 30, train loss: 0.041\n",
      "==>>> epoch: 30, validation loss: 0.081, accuracy: 97.467%\n",
      "==>>> epoch: 31, train loss: 0.039\n",
      "==>>> epoch: 31, validation loss: 0.081, accuracy: 97.450%\n",
      "==>>> epoch: 32, train loss: 0.037\n",
      "==>>> epoch: 32, validation loss: 0.079, accuracy: 97.467%\n",
      "==>>> epoch: 33, train loss: 0.035\n",
      "==>>> epoch: 33, validation loss: 0.078, accuracy: 97.567%\n",
      "==>>> epoch: 34, train loss: 0.033\n",
      "==>>> epoch: 34, validation loss: 0.076, accuracy: 97.633%\n",
      "==>>> epoch: 35, train loss: 0.031\n",
      "==>>> epoch: 35, validation loss: 0.078, accuracy: 97.567%\n",
      "==>>> epoch: 36, train loss: 0.030\n",
      "==>>> epoch: 36, validation loss: 0.078, accuracy: 97.567%\n",
      "==>>> epoch: 37, train loss: 0.028\n",
      "==>>> epoch: 37, validation loss: 0.072, accuracy: 97.767%\n",
      "==>>> epoch: 38, train loss: 0.026\n",
      "==>>> epoch: 38, validation loss: 0.073, accuracy: 97.700%\n",
      "==>>> epoch: 39, train loss: 0.025\n",
      "==>>> epoch: 39, validation loss: 0.073, accuracy: 97.650%\n",
      "==>>> epoch: 40, train loss: 0.023\n",
      "==>>> epoch: 40, validation loss: 0.073, accuracy: 97.650%\n",
      "==>>> epoch: 41, train loss: 0.023\n",
      "==>>> epoch: 41, validation loss: 0.072, accuracy: 97.750%\n",
      "==>>> epoch: 42, train loss: 0.021\n",
      "==>>> epoch: 42, validation loss: 0.071, accuracy: 97.767%\n",
      "==>>> epoch: 43, train loss: 0.020\n",
      "==>>> epoch: 43, validation loss: 0.071, accuracy: 97.783%\n",
      "==>>> epoch: 44, train loss: 0.019\n",
      "==>>> epoch: 44, validation loss: 0.075, accuracy: 97.483%\n",
      "==>>> epoch: 45, train loss: 0.018\n",
      "==>>> epoch: 45, validation loss: 0.070, accuracy: 97.617%\n",
      "==>>> epoch: 46, train loss: 0.017\n",
      "==>>> epoch: 46, validation loss: 0.071, accuracy: 97.767%\n",
      "==>>> epoch: 47, train loss: 0.016\n",
      "==>>> epoch: 47, validation loss: 0.068, accuracy: 97.783%\n",
      "==>>> epoch: 48, train loss: 0.015\n",
      "==>>> epoch: 48, validation loss: 0.071, accuracy: 97.783%\n",
      "==>>> epoch: 49, train loss: 0.015\n",
      "==>>> epoch: 49, validation loss: 0.070, accuracy: 97.750%\n",
      "==>>> epoch: 50, train loss: 0.014\n",
      "==>>> epoch: 50, validation loss: 0.070, accuracy: 97.717%\n",
      "==>>> epoch: 51, train loss: 0.014\n",
      "==>>> epoch: 51, validation loss: 0.069, accuracy: 97.750%\n",
      "==>>> epoch: 52, train loss: 0.013\n",
      "==>>> epoch: 52, validation loss: 0.068, accuracy: 97.833%\n",
      "==>>> epoch: 53, train loss: 0.012\n",
      "==>>> epoch: 53, validation loss: 0.069, accuracy: 97.733%\n",
      "==>>> epoch: 54, train loss: 0.012\n",
      "==>>> epoch: 54, validation loss: 0.068, accuracy: 97.733%\n",
      "==>>> epoch: 55, train loss: 0.011\n",
      "==>>> epoch: 55, validation loss: 0.067, accuracy: 97.867%\n",
      "==>>> epoch: 56, train loss: 0.011\n",
      "==>>> epoch: 56, validation loss: 0.069, accuracy: 97.733%\n",
      "==>>> epoch: 57, train loss: 0.010\n",
      "==>>> epoch: 57, validation loss: 0.069, accuracy: 97.783%\n",
      "==>>> epoch: 58, train loss: 0.010\n",
      "==>>> epoch: 58, validation loss: 0.068, accuracy: 97.783%\n",
      "==>>> epoch: 59, train loss: 0.009\n",
      "==>>> epoch: 59, validation loss: 0.068, accuracy: 97.833%\n",
      "==>>> epoch: 60, train loss: 0.009\n",
      "==>>> epoch: 60, validation loss: 0.068, accuracy: 97.867%\n",
      "==>>> epoch: 61, train loss: 0.009\n",
      "==>>> epoch: 61, validation loss: 0.068, accuracy: 97.800%\n",
      "==>>> epoch: 62, train loss: 0.008\n",
      "==>>> epoch: 62, validation loss: 0.070, accuracy: 97.817%\n",
      "==>>> epoch: 63, train loss: 0.008\n",
      "==>>> epoch: 63, validation loss: 0.070, accuracy: 97.767%\n",
      "==>>> epoch: 64, train loss: 0.008\n",
      "==>>> epoch: 64, validation loss: 0.068, accuracy: 97.867%\n",
      "==>>> epoch: 65, train loss: 0.007\n",
      "==>>> epoch: 65, validation loss: 0.069, accuracy: 97.883%\n",
      "==>>> epoch: 66, train loss: 0.007\n",
      "==>>> epoch: 66, validation loss: 0.069, accuracy: 97.817%\n",
      "==>>> epoch: 67, train loss: 0.007\n",
      "==>>> epoch: 67, validation loss: 0.068, accuracy: 97.783%\n",
      "==>>> epoch: 68, train loss: 0.007\n",
      "==>>> epoch: 68, validation loss: 0.069, accuracy: 97.800%\n",
      "==>>> epoch: 69, train loss: 0.006\n",
      "==>>> epoch: 69, validation loss: 0.068, accuracy: 97.817%\n",
      "==>>> epoch: 70, train loss: 0.006\n",
      "==>>> epoch: 70, validation loss: 0.068, accuracy: 97.817%\n",
      "==>>> epoch: 71, train loss: 0.006\n",
      "==>>> epoch: 71, validation loss: 0.070, accuracy: 97.833%\n",
      "==>>> epoch: 72, train loss: 0.006\n",
      "==>>> epoch: 72, validation loss: 0.068, accuracy: 97.783%\n",
      "==>>> epoch: 73, train loss: 0.006\n",
      "==>>> epoch: 73, validation loss: 0.068, accuracy: 97.833%\n",
      "==>>> epoch: 74, train loss: 0.005\n",
      "==>>> epoch: 74, validation loss: 0.068, accuracy: 97.817%\n",
      "==>>> epoch: 75, train loss: 0.005\n",
      "==>>> epoch: 75, validation loss: 0.068, accuracy: 97.817%\n",
      "==>>> epoch: 76, train loss: 0.005\n",
      "==>>> epoch: 76, validation loss: 0.069, accuracy: 97.767%\n",
      "==>>> epoch: 77, train loss: 0.005\n",
      "==>>> epoch: 77, validation loss: 0.068, accuracy: 97.817%\n",
      "==>>> epoch: 78, train loss: 0.005\n",
      "==>>> epoch: 78, validation loss: 0.068, accuracy: 97.817%\n",
      "==>>> epoch: 79, train loss: 0.005\n",
      "==>>> epoch: 79, validation loss: 0.068, accuracy: 97.800%\n",
      "==>>> epoch: 80, train loss: 0.004\n",
      "==>>> epoch: 80, validation loss: 0.068, accuracy: 97.900%\n",
      "==>>> epoch: 81, train loss: 0.004\n",
      "==>>> epoch: 81, validation loss: 0.069, accuracy: 97.783%\n",
      "==>>> epoch: 82, train loss: 0.004\n",
      "==>>> epoch: 82, validation loss: 0.069, accuracy: 97.850%\n",
      "==>>> epoch: 83, train loss: 0.004\n",
      "==>>> epoch: 83, validation loss: 0.069, accuracy: 97.833%\n",
      "==>>> epoch: 84, train loss: 0.004\n",
      "==>>> epoch: 84, validation loss: 0.070, accuracy: 97.900%\n",
      "==>>> epoch: 85, train loss: 0.004\n",
      "==>>> epoch: 85, validation loss: 0.069, accuracy: 97.933%\n",
      "==>>> epoch: 86, train loss: 0.004\n",
      "==>>> epoch: 86, validation loss: 0.068, accuracy: 97.900%\n",
      "==>>> epoch: 87, train loss: 0.004\n",
      "==>>> epoch: 87, validation loss: 0.068, accuracy: 97.950%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 88, train loss: 0.004\n",
      "==>>> epoch: 88, validation loss: 0.070, accuracy: 97.883%\n",
      "==>>> epoch: 89, train loss: 0.004\n",
      "==>>> epoch: 89, validation loss: 0.070, accuracy: 97.900%\n",
      "==>>> epoch: 90, train loss: 0.003\n",
      "==>>> epoch: 90, validation loss: 0.070, accuracy: 97.917%\n",
      "==>>> epoch: 91, train loss: 0.003\n",
      "==>>> epoch: 91, validation loss: 0.070, accuracy: 97.933%\n",
      "==>>> epoch: 92, train loss: 0.003\n",
      "==>>> epoch: 92, validation loss: 0.069, accuracy: 97.833%\n",
      "==>>> epoch: 93, train loss: 0.003\n",
      "==>>> epoch: 93, validation loss: 0.069, accuracy: 97.900%\n",
      "==>>> epoch: 94, train loss: 0.003\n",
      "==>>> epoch: 94, validation loss: 0.070, accuracy: 97.867%\n",
      "==>>> epoch: 95, train loss: 0.003\n",
      "==>>> epoch: 95, validation loss: 0.070, accuracy: 97.850%\n",
      "==>>> epoch: 96, train loss: 0.003\n",
      "==>>> epoch: 96, validation loss: 0.070, accuracy: 97.917%\n",
      "==>>> epoch: 97, train loss: 0.003\n",
      "==>>> epoch: 97, validation loss: 0.069, accuracy: 97.883%\n",
      "==>>> epoch: 98, train loss: 0.003\n",
      "==>>> epoch: 98, validation loss: 0.070, accuracy: 97.933%\n",
      "==>>> epoch: 99, train loss: 0.003\n",
      "==>>> epoch: 99, validation loss: 0.069, accuracy: 97.900%\n"
     ]
    }
   ],
   "source": [
    "## training\n",
    "model = NN(300, 200)\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "acc_max = 0.0\n",
    "train_loss_epochs = []\n",
    "val_loss_epochs = [] \n",
    "val_acc_epochs = []\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "    # trainning\n",
    "    train_loss = 0\n",
    "    count = 0 \n",
    "    for x, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        if use_cuda:\n",
    "            x, target = x.cuda(), target.cuda()\n",
    "            \n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        train_loss += loss.data\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "    train_loss /= count \n",
    "    \n",
    "    # validation\n",
    "    val_loss = 0\n",
    "    correct_count = 0\n",
    "    tot_count = 0 \n",
    "    count = 0 \n",
    "    with torch.no_grad():\n",
    "        for x, target in val_loader:\n",
    "            if use_cuda:\n",
    "                x, target = x.cuda(), target.cuda()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, target)\n",
    "            val_loss += loss.data\n",
    "            count += 1 \n",
    "            \n",
    "            _, pred_label = torch.max(out.data, 1)\n",
    "            correct_count += (pred_label == target.data).sum()\n",
    "            tot_count += pred_label.shape[0]\n",
    "            \n",
    "    val_loss /= count         \n",
    "    acc = correct_count / tot_count * 100 \n",
    "    print('==>>> epoch: {}, train loss: {:.3f}'.format(epoch, train_loss))\n",
    "    print('==>>> epoch: {}, validation loss: {:.3f}, accuracy: {:.3f}%'.format(epoch, val_loss, acc)) \n",
    "    \n",
    "    train_loss_epochs.append(train_loss.detach().cpu().numpy())\n",
    "    val_loss_epochs.append(val_loss.detach().cpu().numpy())\n",
    "    val_acc_epochs.append(acc.detach().cpu().numpy())\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(train_loss_epochs, label='Training loss')\n",
    "    plt.plot(val_loss_epochs, label='Validation loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.rc('font', size=18) # controls default text sizes\n",
    "\n",
    "    plt.savefig('./figures/model_loss_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(val_acc_epochs, label='Validation accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.rc('font', size=18)\n",
    "    plt.savefig('./figures/model_accuracy_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    if acc > acc_max:\n",
    "        acc_max = acc \n",
    "        torch.save(model.state_dict(), './model_weights/model_best.pth.tar')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> Testing accuracy 98.190%\n",
      "==>>> Testing error 1.810%\n"
     ]
    }
   ],
   "source": [
    "# Testing     \n",
    "# To load model weights\n",
    "#model.load_state_dict(torch.load('./model_weights/model_best.pth.tar'))\n",
    "correct_count = 0\n",
    "wrong_count = 0\n",
    "tot_count = 0\n",
    "with torch.no_grad():\n",
    "        for x, target in test_loader:\n",
    "            if use_cuda:\n",
    "                x, target = x.cuda(), target.cuda()\n",
    "            out = model(x)\n",
    "            _, pred_label = torch.max(out, 1)\n",
    "            correct_count += (pred_label == target.data).sum()\n",
    "            wrong_count += (pred_label != target.data).sum() \n",
    "            tot_count += pred_label.shape[0]\n",
    "    \n",
    "test_acc = correct_count / tot_count * 100 \n",
    "test_err = wrong_count / tot_count * 100 \n",
    "print('==>>> Testing accuracy {:.3f}%'.format(test_acc)) \n",
    "print('==>>> Testing error {:.3f}%'.format(test_err)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All weights initialized to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, train loss: 2.306\n",
      "==>>> epoch: 0, validation loss: 2.306, accuracy: 9.317%\n",
      "==>>> epoch: 1, train loss: 2.306\n",
      "==>>> epoch: 1, validation loss: 2.308, accuracy: 11.300%\n",
      "==>>> epoch: 2, train loss: 2.305\n",
      "==>>> epoch: 2, validation loss: 2.312, accuracy: 9.950%\n",
      "==>>> epoch: 3, train loss: 2.306\n",
      "==>>> epoch: 3, validation loss: 2.307, accuracy: 9.700%\n",
      "==>>> epoch: 4, train loss: 2.305\n",
      "==>>> epoch: 4, validation loss: 2.316, accuracy: 10.717%\n",
      "==>>> epoch: 5, train loss: 2.305\n",
      "==>>> epoch: 5, validation loss: 2.308, accuracy: 9.933%\n",
      "==>>> epoch: 6, train loss: 2.305\n",
      "==>>> epoch: 6, validation loss: 2.305, accuracy: 10.183%\n",
      "==>>> epoch: 7, train loss: 2.305\n",
      "==>>> epoch: 7, validation loss: 2.304, accuracy: 11.300%\n",
      "==>>> epoch: 8, train loss: 2.305\n",
      "==>>> epoch: 8, validation loss: 2.305, accuracy: 9.933%\n",
      "==>>> epoch: 9, train loss: 2.304\n",
      "==>>> epoch: 9, validation loss: 2.304, accuracy: 11.300%\n",
      "==>>> epoch: 10, train loss: 2.304\n",
      "==>>> epoch: 10, validation loss: 2.304, accuracy: 9.733%\n",
      "==>>> epoch: 11, train loss: 2.304\n",
      "==>>> epoch: 11, validation loss: 2.307, accuracy: 9.317%\n",
      "==>>> epoch: 12, train loss: 2.304\n",
      "==>>> epoch: 12, validation loss: 2.307, accuracy: 10.717%\n",
      "==>>> epoch: 13, train loss: 2.305\n",
      "==>>> epoch: 13, validation loss: 2.304, accuracy: 11.300%\n",
      "==>>> epoch: 14, train loss: 2.304\n",
      "==>>> epoch: 14, validation loss: 2.311, accuracy: 9.700%\n",
      "==>>> epoch: 15, train loss: 2.304\n",
      "==>>> epoch: 15, validation loss: 2.305, accuracy: 11.300%\n",
      "==>>> epoch: 16, train loss: 2.304\n",
      "==>>> epoch: 16, validation loss: 2.304, accuracy: 11.300%\n",
      "==>>> epoch: 17, train loss: 2.304\n",
      "==>>> epoch: 17, validation loss: 2.303, accuracy: 11.300%\n",
      "==>>> epoch: 18, train loss: 2.303\n",
      "==>>> epoch: 18, validation loss: 2.304, accuracy: 11.300%\n",
      "==>>> epoch: 19, train loss: 2.304\n",
      "==>>> epoch: 19, validation loss: 2.302, accuracy: 11.300%\n",
      "==>>> epoch: 20, train loss: 2.303\n",
      "==>>> epoch: 20, validation loss: 2.305, accuracy: 10.717%\n",
      "==>>> epoch: 21, train loss: 2.303\n",
      "==>>> epoch: 21, validation loss: 2.306, accuracy: 9.317%\n",
      "==>>> epoch: 22, train loss: 2.303\n",
      "==>>> epoch: 22, validation loss: 2.302, accuracy: 10.717%\n",
      "==>>> epoch: 23, train loss: 2.303\n",
      "==>>> epoch: 23, validation loss: 2.306, accuracy: 9.317%\n",
      "==>>> epoch: 24, train loss: 2.302\n",
      "==>>> epoch: 24, validation loss: 2.302, accuracy: 11.300%\n",
      "==>>> epoch: 25, train loss: 2.302\n",
      "==>>> epoch: 25, validation loss: 2.305, accuracy: 9.317%\n",
      "==>>> epoch: 26, train loss: 2.301\n",
      "==>>> epoch: 26, validation loss: 2.300, accuracy: 10.717%\n",
      "==>>> epoch: 27, train loss: 2.300\n",
      "==>>> epoch: 27, validation loss: 2.299, accuracy: 11.300%\n",
      "==>>> epoch: 28, train loss: 2.298\n",
      "==>>> epoch: 28, validation loss: 2.297, accuracy: 11.300%\n",
      "==>>> epoch: 29, train loss: 2.294\n",
      "==>>> epoch: 29, validation loss: 2.291, accuracy: 11.300%\n",
      "==>>> epoch: 30, train loss: 2.285\n",
      "==>>> epoch: 30, validation loss: 2.277, accuracy: 11.300%\n",
      "==>>> epoch: 31, train loss: 2.260\n",
      "==>>> epoch: 31, validation loss: 2.237, accuracy: 11.883%\n",
      "==>>> epoch: 32, train loss: 2.194\n",
      "==>>> epoch: 32, validation loss: 2.140, accuracy: 21.900%\n",
      "==>>> epoch: 33, train loss: 2.089\n",
      "==>>> epoch: 33, validation loss: 2.045, accuracy: 25.750%\n",
      "==>>> epoch: 34, train loss: 2.015\n",
      "==>>> epoch: 34, validation loss: 1.992, accuracy: 27.167%\n",
      "==>>> epoch: 35, train loss: 1.972\n",
      "==>>> epoch: 35, validation loss: 1.955, accuracy: 28.417%\n",
      "==>>> epoch: 36, train loss: 1.938\n",
      "==>>> epoch: 36, validation loss: 1.923, accuracy: 29.433%\n",
      "==>>> epoch: 37, train loss: 1.906\n",
      "==>>> epoch: 37, validation loss: 1.889, accuracy: 29.983%\n",
      "==>>> epoch: 38, train loss: 1.875\n",
      "==>>> epoch: 38, validation loss: 1.858, accuracy: 30.667%\n",
      "==>>> epoch: 39, train loss: 1.845\n",
      "==>>> epoch: 39, validation loss: 1.830, accuracy: 31.283%\n",
      "==>>> epoch: 40, train loss: 1.818\n",
      "==>>> epoch: 40, validation loss: 1.804, accuracy: 30.600%\n",
      "==>>> epoch: 41, train loss: 1.795\n",
      "==>>> epoch: 41, validation loss: 1.781, accuracy: 32.200%\n",
      "==>>> epoch: 42, train loss: 1.775\n",
      "==>>> epoch: 42, validation loss: 1.762, accuracy: 32.000%\n",
      "==>>> epoch: 43, train loss: 1.758\n",
      "==>>> epoch: 43, validation loss: 1.747, accuracy: 31.567%\n",
      "==>>> epoch: 44, train loss: 1.743\n",
      "==>>> epoch: 44, validation loss: 1.733, accuracy: 33.500%\n",
      "==>>> epoch: 45, train loss: 1.729\n",
      "==>>> epoch: 45, validation loss: 1.720, accuracy: 32.600%\n",
      "==>>> epoch: 46, train loss: 1.718\n",
      "==>>> epoch: 46, validation loss: 1.710, accuracy: 33.483%\n",
      "==>>> epoch: 47, train loss: 1.708\n",
      "==>>> epoch: 47, validation loss: 1.699, accuracy: 34.000%\n",
      "==>>> epoch: 48, train loss: 1.698\n",
      "==>>> epoch: 48, validation loss: 1.690, accuracy: 34.000%\n",
      "==>>> epoch: 49, train loss: 1.690\n",
      "==>>> epoch: 49, validation loss: 1.682, accuracy: 34.450%\n",
      "==>>> epoch: 50, train loss: 1.683\n",
      "==>>> epoch: 50, validation loss: 1.676, accuracy: 34.967%\n",
      "==>>> epoch: 51, train loss: 1.676\n",
      "==>>> epoch: 51, validation loss: 1.670, accuracy: 35.050%\n",
      "==>>> epoch: 52, train loss: 1.670\n",
      "==>>> epoch: 52, validation loss: 1.663, accuracy: 34.883%\n",
      "==>>> epoch: 53, train loss: 1.664\n",
      "==>>> epoch: 53, validation loss: 1.657, accuracy: 34.917%\n",
      "==>>> epoch: 54, train loss: 1.659\n",
      "==>>> epoch: 54, validation loss: 1.652, accuracy: 35.400%\n",
      "==>>> epoch: 55, train loss: 1.654\n",
      "==>>> epoch: 55, validation loss: 1.647, accuracy: 36.217%\n",
      "==>>> epoch: 56, train loss: 1.649\n",
      "==>>> epoch: 56, validation loss: 1.643, accuracy: 34.917%\n",
      "==>>> epoch: 57, train loss: 1.644\n",
      "==>>> epoch: 57, validation loss: 1.638, accuracy: 34.917%\n",
      "==>>> epoch: 58, train loss: 1.640\n",
      "==>>> epoch: 58, validation loss: 1.634, accuracy: 35.350%\n",
      "==>>> epoch: 59, train loss: 1.637\n",
      "==>>> epoch: 59, validation loss: 1.630, accuracy: 35.950%\n",
      "==>>> epoch: 60, train loss: 1.633\n",
      "==>>> epoch: 60, validation loss: 1.627, accuracy: 35.567%\n",
      "==>>> epoch: 61, train loss: 1.629\n",
      "==>>> epoch: 61, validation loss: 1.623, accuracy: 35.467%\n",
      "==>>> epoch: 62, train loss: 1.626\n",
      "==>>> epoch: 62, validation loss: 1.620, accuracy: 35.133%\n",
      "==>>> epoch: 63, train loss: 1.623\n",
      "==>>> epoch: 63, validation loss: 1.617, accuracy: 36.717%\n",
      "==>>> epoch: 64, train loss: 1.620\n",
      "==>>> epoch: 64, validation loss: 1.613, accuracy: 36.600%\n",
      "==>>> epoch: 65, train loss: 1.617\n",
      "==>>> epoch: 65, validation loss: 1.611, accuracy: 36.033%\n",
      "==>>> epoch: 66, train loss: 1.615\n",
      "==>>> epoch: 66, validation loss: 1.608, accuracy: 36.133%\n",
      "==>>> epoch: 67, train loss: 1.612\n",
      "==>>> epoch: 67, validation loss: 1.607, accuracy: 36.667%\n",
      "==>>> epoch: 68, train loss: 1.610\n",
      "==>>> epoch: 68, validation loss: 1.604, accuracy: 37.150%\n",
      "==>>> epoch: 69, train loss: 1.607\n",
      "==>>> epoch: 69, validation loss: 1.603, accuracy: 35.733%\n",
      "==>>> epoch: 70, train loss: 1.606\n",
      "==>>> epoch: 70, validation loss: 1.600, accuracy: 36.350%\n",
      "==>>> epoch: 71, train loss: 1.604\n",
      "==>>> epoch: 71, validation loss: 1.598, accuracy: 36.133%\n",
      "==>>> epoch: 72, train loss: 1.601\n",
      "==>>> epoch: 72, validation loss: 1.596, accuracy: 36.400%\n",
      "==>>> epoch: 73, train loss: 1.599\n",
      "==>>> epoch: 73, validation loss: 1.593, accuracy: 36.600%\n",
      "==>>> epoch: 74, train loss: 1.597\n",
      "==>>> epoch: 74, validation loss: 1.589, accuracy: 36.667%\n",
      "==>>> epoch: 75, train loss: 1.592\n",
      "==>>> epoch: 75, validation loss: 1.585, accuracy: 36.450%\n",
      "==>>> epoch: 76, train loss: 1.586\n",
      "==>>> epoch: 76, validation loss: 1.577, accuracy: 36.950%\n",
      "==>>> epoch: 77, train loss: 1.579\n",
      "==>>> epoch: 77, validation loss: 1.571, accuracy: 37.550%\n",
      "==>>> epoch: 78, train loss: 1.570\n",
      "==>>> epoch: 78, validation loss: 1.562, accuracy: 37.483%\n",
      "==>>> epoch: 79, train loss: 1.562\n",
      "==>>> epoch: 79, validation loss: 1.554, accuracy: 38.300%\n",
      "==>>> epoch: 80, train loss: 1.555\n",
      "==>>> epoch: 80, validation loss: 1.547, accuracy: 38.833%\n",
      "==>>> epoch: 81, train loss: 1.548\n",
      "==>>> epoch: 81, validation loss: 1.540, accuracy: 38.583%\n",
      "==>>> epoch: 82, train loss: 1.541\n",
      "==>>> epoch: 82, validation loss: 1.532, accuracy: 38.600%\n",
      "==>>> epoch: 83, train loss: 1.535\n",
      "==>>> epoch: 83, validation loss: 1.527, accuracy: 39.383%\n",
      "==>>> epoch: 84, train loss: 1.528\n",
      "==>>> epoch: 84, validation loss: 1.520, accuracy: 39.150%\n",
      "==>>> epoch: 85, train loss: 1.521\n",
      "==>>> epoch: 85, validation loss: 1.512, accuracy: 39.950%\n",
      "==>>> epoch: 86, train loss: 1.514\n",
      "==>>> epoch: 86, validation loss: 1.505, accuracy: 40.250%\n",
      "==>>> epoch: 87, train loss: 1.506\n",
      "==>>> epoch: 87, validation loss: 1.497, accuracy: 40.000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 88, train loss: 1.498\n",
      "==>>> epoch: 88, validation loss: 1.488, accuracy: 40.850%\n",
      "==>>> epoch: 89, train loss: 1.487\n",
      "==>>> epoch: 89, validation loss: 1.476, accuracy: 41.883%\n",
      "==>>> epoch: 90, train loss: 1.476\n",
      "==>>> epoch: 90, validation loss: 1.465, accuracy: 42.333%\n",
      "==>>> epoch: 91, train loss: 1.462\n",
      "==>>> epoch: 91, validation loss: 1.451, accuracy: 42.883%\n",
      "==>>> epoch: 92, train loss: 1.448\n",
      "==>>> epoch: 92, validation loss: 1.437, accuracy: 43.933%\n",
      "==>>> epoch: 93, train loss: 1.432\n",
      "==>>> epoch: 93, validation loss: 1.418, accuracy: 45.067%\n",
      "==>>> epoch: 94, train loss: 1.414\n",
      "==>>> epoch: 94, validation loss: 1.400, accuracy: 46.967%\n",
      "==>>> epoch: 95, train loss: 1.396\n",
      "==>>> epoch: 95, validation loss: 1.381, accuracy: 46.700%\n",
      "==>>> epoch: 96, train loss: 1.376\n",
      "==>>> epoch: 96, validation loss: 1.362, accuracy: 47.250%\n",
      "==>>> epoch: 97, train loss: 1.357\n",
      "==>>> epoch: 97, validation loss: 1.344, accuracy: 48.500%\n",
      "==>>> epoch: 98, train loss: 1.338\n",
      "==>>> epoch: 98, validation loss: 1.326, accuracy: 48.983%\n",
      "==>>> epoch: 99, train loss: 1.319\n",
      "==>>> epoch: 99, validation loss: 1.307, accuracy: 49.917%\n"
     ]
    }
   ],
   "source": [
    "# network\n",
    "class NN_self_zero_init(): \n",
    "    def __init__(self, d1, d2, alpha, out_class=10):\n",
    "        self.W1 = torch.zeros(d1, 28*28) \n",
    "        self.W2 = torch.zeros(d2, d1) \n",
    "        self.W3 = torch.zeros(out_class, d2) \n",
    "        self.b1 = torch.zeros(1, d1)\n",
    "        self.b2 = torch.zeros(1, d2)\n",
    "        self.b3 = torch.zeros(1, out_class)\n",
    "        self.alpha = alpha # learning rate \n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "       \n",
    "    def softmax(self, x):\n",
    "        beta = torch.max(x, dim=1, keepdim=True)[0] # avoid overflow \n",
    "        x_exp = torch.exp(x-beta)\n",
    "        x_exp_sum = torch.sum(x_exp, 1, keepdim=True)\n",
    "        return x_exp / x_exp_sum\n",
    "    \n",
    "    def cross_entropy(self, yhat, ytrue):\n",
    "        return - yhat[range(ytrue.shape[0]), ytrue].log().mean()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28) # batch*(28*28)\n",
    "        z1 = self.sigmoid(torch.matmul(x, self.W1.T) + self.b1) \n",
    "        z2 = self.sigmoid(torch.matmul(z1, self.W2.T) + self.b2) \n",
    "        out = self.softmax(torch.matmul(z2, self.W3.T) + self.b3)\n",
    "        return out, z2, z1, x\n",
    "    \n",
    "    def backward(self, out, ref, z2, z1, x):\n",
    "        batchsize = out.shape[0]\n",
    "        dz3 = - (ref - out) # z3 = output \n",
    "        db3 = torch.mean(dz3, dim=0, keepdim=True) \n",
    "        dW3 = torch.matmul(dz3.T, z2) / batchsize # normalized by the batch size \n",
    "        \n",
    "        dz2 = torch.matmul(dz3, self.W3) * z2 * (1 - z2)\n",
    "        db2 = torch.mean(dz2, dim=0, keepdim=True)\n",
    "        dW2 = torch.matmul(dz2.T, z1) / batchsize\n",
    "        \n",
    "        dz1 = torch.matmul(dz2, self.W2) * z1 * (1 - z1)\n",
    "        db1 = torch.mean(dz1, dim=0, keepdim=True)\n",
    "        dW1 = torch.matmul(dz1.T, x) / batchsize\n",
    "        \n",
    "        self.b3 -= self.alpha * db3\n",
    "        self.W3 -= self.alpha * dW3\n",
    "        self.b2 -= self.alpha * db2\n",
    "        self.W2 -= self.alpha * dW2\n",
    "        self.b1 -= self.alpha * db1\n",
    "        self.W1 -= self.alpha * dW1\n",
    "        \n",
    "acc_max = 0.0\n",
    "#criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "## training\n",
    "model_self = NN_self_zero_init(300, 200, alpha=0.2) # alpha/learning rate \n",
    "train_loss_epochs = []\n",
    "val_loss_epochs = [] \n",
    "val_acc_epochs = []\n",
    "for epoch in range(nepoch):\n",
    "    # trainning\n",
    "    train_loss = 0\n",
    "    count = 0 \n",
    "    for x, target in train_loader:\n",
    "        out, z2, z1, x = model_self.forward(x)\n",
    "        loss = model_self.cross_entropy(out, target) \n",
    "        #loss = F.cross_entropy(out.log(), target) # cross entropy loss accepts logits rather than softmax output \n",
    "        train_loss += loss\n",
    "        # to one-hot \n",
    "        ref = F.one_hot(target, num_classes=10).to(out)\n",
    "        model_self.backward(out, ref, z2, z1, x)\n",
    "        count += 1\n",
    "        \n",
    "    train_loss /= count \n",
    "    # validation\n",
    "    val_loss = 0\n",
    "    correct_count = 0\n",
    "    tot_count = 0 \n",
    "    count = 0 \n",
    "    for x, target in val_loader:\n",
    "        out, _, _, _ = model_self.forward(x) \n",
    "        loss = model_self.cross_entropy(out, target) \n",
    "        val_loss += loss\n",
    "        count += 1\n",
    "        _, pred_label = torch.max(out, 1)\n",
    "        correct_count += (pred_label == target.data).sum()\n",
    "        tot_count += pred_label.shape[0]\n",
    "    \n",
    "    val_loss /= count \n",
    "    acc = correct_count / tot_count * 100 \n",
    "    print('==>>> epoch: {}, train loss: {:.3f}'.format(epoch, train_loss))\n",
    "    print('==>>> epoch: {}, validation loss: {:.3f}, accuracy: {:.3f}%'.format(epoch, val_loss, acc)) \n",
    "    \n",
    "    train_loss_epochs.append(train_loss)\n",
    "    val_loss_epochs.append(val_loss)\n",
    "    val_acc_epochs.append(acc)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(train_loss_epochs, label='Training loss')\n",
    "    plt.plot(val_loss_epochs, label='Validation loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.rc('font', size=18) # controls default text sizes\n",
    "\n",
    "    plt.savefig('./figures/model_self_zero_init_loss_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(val_acc_epochs, label='Validation accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.rc('font', size=18)\n",
    "    plt.savefig('./figures/model_self_zero_init_accuracy_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    if acc > acc_max:\n",
    "        acc_max = acc \n",
    "        torch.save(model_self, './model_weights/model_self_zero_init_best.pth.tar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> Testing accuracy 49.790%\n",
      "==>>> Testing error 50.210%\n"
     ]
    }
   ],
   "source": [
    "# Testing      \n",
    "model_self = torch.load('./model_weights/model_self_zero_init_best.pth.tar')\n",
    "correct_count = 0\n",
    "wrong_count = 0\n",
    "tot_count = 0\n",
    "for x, target in test_loader:\n",
    "    out, _, _, _ = model_self.forward(x) \n",
    "    _, pred_label = torch.max(out, 1)\n",
    "    correct_count += (pred_label == target.data).sum()\n",
    "    wrong_count += (pred_label != target.data).sum() \n",
    "    tot_count += pred_label.shape[0]\n",
    "    \n",
    "test_acc = correct_count / tot_count * 100 \n",
    "test_err = wrong_count / tot_count * 100 \n",
    "print('==>>> Testing accuracy {:.3f}%'.format(test_acc)) \n",
    "print('==>>> Testing error {:.3f}%'.format(test_err)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
